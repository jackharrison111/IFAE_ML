{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff85496-62e3-4244-ba57-cfc353fc0b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "all_samples_path = '/data/at3/scratch3/jharrison/nominal_feather/Regions'\n",
    "\n",
    "#region_file = 'CR_1Z_0b_2SFOS_newvars.ftr'\n",
    "#signal_feather = 'CR_1Z_0b_2SFOS_VLLs_newvars.ftr'\n",
    "\n",
    "#region_file = 'CR_0Z_0b_2SFOS_newvars.ftr'\n",
    "#signal_feather = 'CR_0Z_0b_2SFOS_VLLs_newvars.ftr'\n",
    "\n",
    "region_file = 'CR_2Z_0b_newvars.ftr'\n",
    "signal_feather = 'CR_2Z_0b_VLLs_newvars.ftr'\n",
    "\n",
    "#Set training hyperparameters\n",
    "model_type = 'VAE'\n",
    "batch_size = 1024\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 20\n",
    "Train = True\n",
    "load_path = 'outputs/VAE_1714-23-08-2022/model_state_dict.pt'\n",
    "test_dump = False\n",
    "size= -1\n",
    "\n",
    "\n",
    "#Preprocessing settings\n",
    "remove_zero_weights = True\n",
    "remove_negative_weights = True\n",
    "weight_loss = True\n",
    "added_weight_factor = 500\n",
    "\n",
    "#Test settings\n",
    "validation_groups = ['VV', 'Top', 'VH', 'VVV']\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.2\n",
    "val_frequency = 2\n",
    "num_test_samples = -1\n",
    "evaluate_signals = True\n",
    "\n",
    "additional_comments = \"\"\"\n",
    "VAE:\n",
    "Testing separations.\n",
    "Using best mZ to start.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13169340-ea28-4fb7-82dc-05f0d4223bc7",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f11fd80-2758-4922-acad-f25b0c8faef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['singleTop',\n",
       " 'singleTop',\n",
       " 'tW',\n",
       " 'Zjets',\n",
       " 'Wjets',\n",
       " 'Vgamma',\n",
       " 'ttW2210',\n",
       " 'ttH',\n",
       " 'ttZalt',\n",
       " 'rareTop',\n",
       " 'VV',\n",
       " 'threeTop',\n",
       " 'fourTop',\n",
       " 'ttWW',\n",
       " 'tZ',\n",
       " 'WtZ',\n",
       " 'VVV',\n",
       " 'VH',\n",
       " 'tHjb',\n",
       " 'ttbar',\n",
       " 'ttZZ',\n",
       " 'ttWH',\n",
       " 'ttHH']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = \"XXX_singleTop_schan_samples, XXX_singleTop_tchan_samples, XXX_tW_samples, XXX_Zjets_samples, XXX_Wjets_samples, XXX_Vgamma_samples, XXX_ttW2210_EW_samples, XXX_ttH_samples, XXX_ttZalt_samples, XXX_rareTop_samples, XXX_VV_samples, XXX_threeTop_samples, XXX_fourTop_samples, XXX_ttWW_samples, XXX_tZ_samples, XXX_WtZ_samples, XXX_VVV_samples, XXX_VH_samples,XXX_tHjb_samples,XXX_ttbar_nonallhad_samples,XXX_ttZZ_samples,XXX_ttWH_samples,XXX_ttHH_samples\"\n",
    "\n",
    "samples = all.split(',')\n",
    "sample_names = [s.split('_')[1] for s in samples]\n",
    "sample_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8bc0025-6769-4942-aeaa-755c9df117ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.read_feather(os.path.join(all_samples_path, region_file))\n",
    "\n",
    "#chosen_samples = ['ggVV', 'singleTop', 'tW', 'Zjets', 'Wjets', 'Vgamma', 'ttW2210', 'ttH', 'ttZalt', 'rareTop', 'VV', \n",
    "#                  'threeTop', 'fourTop', 'ttWW',\n",
    "#                  'tZ', 'WtZ', 'VVV', 'VH', 'tHjb', 'ttbar', 'ttZZ', 'ttWH', 'ttHH']\n",
    "\n",
    "chosen_samples =['singleTop',\n",
    " 'tW',\n",
    " 'Zjets',\n",
    " 'Wjets',\n",
    " 'Vgamma',\n",
    " 'ttW2210EW',\n",
    " 'ttH',\n",
    " 'ttZalt',\n",
    " 'rareTop',\n",
    " 'VV',\n",
    " 'ggVV',\n",
    " 'threeTop',\n",
    " 'fourTop',\n",
    " 'ttWW',\n",
    " 'tZ',\n",
    " 'WtZ',\n",
    " 'VVV',\n",
    " 'VH',\n",
    " 'tHjb',\n",
    " 'ttbarnonallhad',\n",
    " 'ttZZ',\n",
    " 'ttWH',\n",
    " 'ttHH']\n",
    "\n",
    "groupings = {\n",
    "    'VV' : ['ggVV','VV'],\n",
    "    'VVV': ['VVV'],\n",
    "    'VH' : ['VH'],\n",
    "    'Top': ['ttZalt', 'fourTop', 'ttWW','ttH','WtZ'],\n",
    "    'Other' : ['singleTop','tW','Zjets','Wjets','Vgamma','ttW2210EW','rareTop','threeTop','tZ','tHjb','ttbarnonallhad','ttZZ','ttWH','ttHH']\n",
    "}\n",
    "reversed_groupings = {}\n",
    "for key, values in groupings.items():\n",
    "    for val in values:\n",
    "        reversed_groupings[val] = key\n",
    "\n",
    "training_variables = [\n",
    "#    'lep_Pt_0',\n",
    "#    'lep_Pt_1',\n",
    "#    'lep_Pt_2',\n",
    "#    'lep_Pt_3',\n",
    "#    'lep_Eta_0',\n",
    "#    'lep_Eta_1',\n",
    "#    'lep_Eta_2',\n",
    "#    'lep_Eta_3',\n",
    "#    'lep_Phi_0',\n",
    "#    'lep_Phi_1',\n",
    "#    'lep_Phi_2',\n",
    "#    #'lep_Phi_3',\n",
    "    'met_met',\n",
    "#    'met_phi',\n",
    "    'Mllll0123',\n",
    "    'HT_lep',\n",
    "    'HT_jets',\n",
    "    'nJets_OR',\n",
    "#    'sumPsbtag',\n",
    "    'weight',\n",
    "    'sample',\n",
    "#    'Mll01',\n",
    "#    'Mll02',\n",
    "#    'Mll03',\n",
    "#    'Mll12',\n",
    "#    'Mll13',\n",
    "#    'Mll23',\n",
    "    'best_mZll',\n",
    "    'other_mZll',\n",
    "    'M3l_high',\n",
    "    'M3l_low',\n",
    "    'best_ptZll',\n",
    "    'other_ptZll'\n",
    "]\n",
    "get_bestZ = False\n",
    "#We will want Mll of SFOS ? \n",
    "\n",
    "mc_data = data.loc[data['sample'].isin(chosen_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99826d-86cc-42e4-83cf-2b8525bde291",
   "metadata": {},
   "source": [
    "# Calculate weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f84d46-275a-4bae-a6e1-c02aadd37e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9633/2084566788.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mc_data.loc[mc_data['RunYear'].isin([2015,2016]), 'lumi_scale'] = 36207.66*(1/total_lum)\n",
      "/tmp/ipykernel_9633/2084566788.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mc_data['weight'] = mc_data['lumi_scale']*mc_data['custTrigSF_TightElMediumMuID_FCLooseIso_DLT']*mc_data['weight_pileup']*mc_data['jvtSF_customOR']*mc_data['bTagSF_weight_DL1r_77']*mc_data['weight_mc']*mc_data['xs']*mc_data['lep_SF_CombinedLoose_0']*mc_data['lep_SF_CombinedLoose_1']*mc_data['lep_SF_CombinedLoose_2']*mc_data['lep_SF_CombinedLoose_3']/mc_data['totalEventsWeighted']\n"
     ]
    }
   ],
   "source": [
    "total_lum = 138965.16\n",
    "mc_data.loc[mc_data['RunYear'].isin([2015,2016]), 'lumi_scale'] = 36207.66*(1/total_lum)\n",
    "mc_data.loc[mc_data['RunYear'].isin([2017]), 'lumi_scale'] = 44307.4*(1/total_lum)\n",
    "mc_data.loc[mc_data['RunYear'].isin([2018]), 'lumi_scale'] = 58450.1*(1/total_lum)\n",
    "\n",
    "mc_data['weight'] = mc_data['lumi_scale']*mc_data['custTrigSF_TightElMediumMuID_FCLooseIso_DLT']*mc_data['weight_pileup']*mc_data['jvtSF_customOR']*mc_data['bTagSF_weight_DL1r_77']*mc_data['weight_mc']*mc_data['xs']*mc_data['lep_SF_CombinedLoose_0']*mc_data['lep_SF_CombinedLoose_1']*mc_data['lep_SF_CombinedLoose_2']*mc_data['lep_SF_CombinedLoose_3']/mc_data['totalEventsWeighted']\n",
    "\n",
    "#mc_data.loc[:,'weight'] = mc_data.loc[:,'lumi_scale']*mc_data.loc[:,'custTrigSF_TightElMediumMuID_FCLooseIso_DLT']*mc_data.loc[:,'weight_pileup']*mc_data.loc[:,'jvtSF_customOR']*mc_data.loc[:,'bTagSF_weight_DL1r_77']*mc_data.loc[:,'weight_mc']*mc_data.loc[:,'xs']*mc_data.loc[:,'lep_SF_CombinedLoose_0']*mc_data.loc[:,'lep_SF_CombinedLoose_1']*mc_data.loc[:,'lep_SF_CombinedLoose_2']*mc_data.loc[:,'lep_SF_CombinedLoose_3']/mc_data.loc[:,'totalEventsWeighted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda046b2-2f62-403f-8b56-5b0da37e4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data = mc_data[training_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e730ef2-5f0a-4a9c-99b0-ebf5fb07a168",
   "metadata": {},
   "source": [
    "# Run over small sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052963ab-0f6e-432c-ad83-0e44f57a203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if size != -1:\n",
    "    mc_data = mc_data[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2305640-e755-4c8d-8918-337874152b20",
   "metadata": {},
   "source": [
    "# Remove negative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6008c6-d20d-4c02-96e1-0a9de31242d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero weights: 6159 , Negative weights: 145831\n"
     ]
    }
   ],
   "source": [
    "num_zero_weights = len(mc_data.loc[mc_data['weight']==0])\n",
    "num_negative_weights = len(mc_data.loc[mc_data['weight']<0])\n",
    "\n",
    "print(f\"Zero weights: {num_zero_weights} , Negative weights: {num_negative_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1494b0bd-4e06-4a77-9ec1-9832b03f25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 0 weights\n",
    "if remove_zero_weights:\n",
    "    mc_data = mc_data.loc[mc_data['weight']!=0]\n",
    "if remove_negative_weights:\n",
    "    mc_data = mc_data.loc[mc_data['weight']>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6949f858-867e-4d72-aaf7-be505ba77103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nulls\n",
    "for col in mc_data.columns:\n",
    "    nulls = mc_data[col].isnull().sum()\n",
    "    if nulls != 0:\n",
    "        print(col, nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78e4c89f-8d8a-4d88-ad06-f737d244f163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 1773216, After: 1773216\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "before = len(mc_data)\n",
    "mc_data = mc_data.drop_duplicates()\n",
    "print(f\"Before: {before}, After: {len(mc_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6212b6-710f-4869-b508-e2823cbf70a5",
   "metadata": {},
   "source": [
    "# Calculate the best mZll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7790f3-f62c-4be9-a26e-bb12f6f8b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Zll_pairing(df, drop_Mlls=False):\n",
    "    \n",
    "    pairings=  {\n",
    "        'Mll01' : 'Mll23', \n",
    "        'Mll02':'Mll13',\n",
    "        'Mll03':'Mll12',\n",
    "        'Mll12':'Mll03',\n",
    "        'Mll13':'Mll02',\n",
    "        'Mll23':'Mll01'\n",
    "         }\n",
    "    mll_columns = list(pairings.keys())\n",
    "    if mll_columns[0] not in df.columns:\n",
    "        print(\"Not found the required Mll columns... Returning the original dataframe.\")\n",
    "        return df\n",
    "    df['bestZpair'] = (abs(df[mll_columns]-91.3e3)).idxmin(axis=1)\n",
    "    df['otherZ'] = df['bestZpair'].map(pairings)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['best_mZll'] = [df.loc[i,col] for i, col in enumerate(df['bestZpair'])]\n",
    "    df['other_mZll'] = [df.loc[i,col] for i, col in enumerate(df['otherZ'])]\n",
    "    \n",
    "    df.drop(['bestZpair','otherZ'], axis=1,inplace=True)\n",
    "    if drop_Mlls:\n",
    "        df.drop(mll_columns,axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "if get_bestZ:\n",
    "    mc_data = get_Zll_pairing(mc_data, drop_Mlls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95024417-7177-41f5-b146-43545ac29a67",
   "metadata": {},
   "source": [
    "# Scaling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24b79a3-54aa-42a5-a489-2cbcab1740f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "met_met -6.893766071790005e-10\n",
      "Mllll0123 8.063392802594596e-09\n",
      "HT_lep 2.972898599236305e-10\n",
      "HT_jets -1.5416660914701734e-09\n",
      "nJets_OR 1.013515935405662e-12\n",
      "best_mZll -8.302928123169035e-10\n",
      "other_mZll -1.7311460626096134e-09\n",
      "M3l_high 2.3497201233162054e-15\n",
      "M3l_low -2.934271000912521e-15\n",
      "best_ptZll 1.280458023151361e-15\n",
      "other_ptZll 2.284962779815135e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "for col in mc_data.columns:\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    if col == 'weight':\n",
    "        #Scale the weights to be centered on 1\n",
    "        mc_data.loc[:,'scaled_weight'] = mc_data.loc[:,col]/mc_data[col].sum()\n",
    "        continue\n",
    "        \n",
    "    if col == 'sample':\n",
    "        continue\n",
    "        \n",
    "    mc_data.loc[:, col] = scaler.fit_transform(np.array(mc_data[col]).reshape(len(mc_data[col]),1))\n",
    "    print(col, mc_data[col].mean())\n",
    "    scalers[col] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7239f522-ff69-4f7f-b7d7-3570e4ca86b0",
   "metadata": {},
   "source": [
    "# Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f691d7-e7b8-4d90-8726-a990ea124635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = train_test_split(mc_data, test_size=validation_fraction, stratify=mc_data['sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9957ed0-6254-4d80-acb6-180cf55127fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mc_data:  1773216\n",
      "Zjets 1\n",
      "ttW2210EW 1\n",
      "ttH 57\n",
      "ttZalt 669\n",
      "rareTop 1\n",
      "VV 119392\n",
      "ggVV 51732\n",
      "threeTop 1\n",
      "fourTop 1\n",
      "tZ 1\n",
      "WtZ 29\n",
      "VVV 5441\n",
      "VH 1\n",
      "ttbarnonallhad 1\n",
      "Removed:  177328\n",
      "Remaining length:  1595888\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "all_val_data = pd.DataFrame()\n",
    "print(\"Length of mc_data: \", len(mc_data))\n",
    "for sample in chosen_samples:\n",
    "    sample_data = mc_data.loc[mc_data['sample'] == sample]\n",
    "    if len(sample_data) == 0:\n",
    "        continue\n",
    "    if len(sample_data) == 1:\n",
    "        continue\n",
    "    train_data, test_data = train_test_split(sample_data, test_size=validation_fraction)\n",
    "    all_val_data = pd.concat([all_val_data, test_data])\n",
    "\n",
    "    mc_data.drop(index=test_data.index.values, axis=0,inplace=True)\n",
    "    print(sample, len(test_data))\n",
    "        \n",
    "print(\"Removed: \", len(all_val_data))\n",
    "print(\"Remaining length: \", len(mc_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0327dda-00b5-4c9a-ac7a-bc3b32fdc289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mc_data:  1595888\n",
      "ggVV 93117\n",
      "VV 214906\n",
      "ttZalt 1204\n",
      "ttH 102\n",
      "WtZ 51\n",
      "VVV 9794\n",
      "Removed:  319174\n",
      "Remaining length:  1276714\n"
     ]
    }
   ],
   "source": [
    "all_test_data = pd.DataFrame()\n",
    "print(\"Length of mc_data: \", len(mc_data))\n",
    "for g in validation_groups:\n",
    "    for sample in groupings[g]:\n",
    "        sample_data = mc_data.loc[mc_data['sample'] == sample]\n",
    "        if len(sample_data) == 0:\n",
    "            continue\n",
    "        if len(sample_data) == 1:\n",
    "            continue\n",
    "        train_data, test_data = train_test_split(sample_data, test_size=test_fraction)\n",
    "        all_test_data = pd.concat([all_test_data, test_data])\n",
    "\n",
    "        mc_data.drop(index=test_data.index.values, axis=0,inplace=True)\n",
    "        print(sample, len(test_data))\n",
    "        \n",
    "print(\"Removed: \", len(all_test_data))\n",
    "print(\"Remaining length: \", len(mc_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6651a-9830-4255-a152-51ff2d14bb0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VAE DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abbfcec4-d6f3-487e-b163-e15abb13213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/at3/scratch3/jharrison/miniconda3/envs/ML_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.autoencoder import VAE, AE\n",
    "\n",
    "useful_columns = [col for col in mc_data.columns if col not in ['sample','weight', 'scaled_weight']]\n",
    "\n",
    "enc_dim = [len(useful_columns),8]\n",
    "dec_dim = [8,len(useful_columns)]\n",
    "z_dim = 4\n",
    "\n",
    "if model_type == 'AE':\n",
    "    model = AE(enc_dim, dec_dim, z_dim)\n",
    "elif model_type == 'VAE':\n",
    "    model = VAE(enc_dim, dec_dim, z_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4225665-eda0-4c3e-836a-6722a85de3bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aa842d7-2f95-461c-b9ec-9afa3cb58f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return reconstruction error + KL divergence losses\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var, variational=True):\n",
    "    #BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    MSE = F.mse_loss(x, recon_x, reduction='none')   #Works it out element-wise\n",
    "    MSE = torch.mean(MSE, dim=1)   #Average across featuers, leaving per-example MSE\n",
    "    \n",
    "    if variational:\n",
    "        KLD = 1 + log_var - mu.pow(2) - log_var.exp()   #Again worked out element-wise\n",
    "        KLD = -0.5 * torch.sum(KLD, dim=1)   #Sum across features, leaving per-example KLD\n",
    "        return MSE + KLD, MSE, KLD\n",
    "    else:\n",
    "        return MSE, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87baa21-1a6a-4c0e-ae1b-57b0061e6f97",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ea639d-3551-4f63-a042-983d47285501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using available: cpu\n"
     ]
    }
   ],
   "source": [
    "#Train on GPU if possible \n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using available: {device}\")\n",
    "device = 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40e89dfb-3e0e-411c-a380-09c7e9ca4dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches per epoch: 1246.791015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of batches per epoch: {len(mc_data) / batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7de6c-0976-45ec-a237-52767d55defe",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "222409fd-9da3-4148-94fc-1263da2b2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd894f-32a2-49b5-9d12-e88216ef2cfb",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "220a729a-7a34-41de-884c-1cf9c94f25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "  \n",
    "# Dataset class, needs to be built from pandas dataframe\n",
    "class data_set(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.weights = None\n",
    "        self.samples = None\n",
    "        self.scaled_weights  = None\n",
    "        \n",
    "        if 'weight' in dataset.columns:\n",
    "            self.weights = torch.tensor(dataset['weight'].values, dtype=torch.float32)\n",
    "            dataset.drop('weight', axis=1, inplace=True)\n",
    "            \n",
    "        if 'scaled_weight' in dataset.columns:\n",
    "            self.scaled_weights = torch.tensor(dataset['scaled_weight'].values, dtype=torch.float32)\n",
    "            dataset.drop('scaled_weight', axis=1, inplace=True)\n",
    "            \n",
    "        if 'sample' in dataset.columns:\n",
    "            self.samples = dataset['sample']\n",
    "            dataset.drop('sample', axis=1, inplace=True)\n",
    "        \n",
    "        self.data = torch.tensor(dataset.values, dtype=torch.float32)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #weights =  self.weights[index] if self.weights!=None else None\n",
    "        #samples =  [] if self.samples == None else self.samples[index]\n",
    "        #TODO: ADD CHECKING FOR IF NO WEIGHTS OR SAMPLES ARE PASSED\n",
    "        sc_weight = self.scaled_weights[index] if self.scaled_weights != None else None\n",
    "        return self.data[index], self.weights[index], self.samples.iloc[index], sc_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea08a275-68e9-4944-aa18-3d876acd8e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_set(mc_data)\n",
    "val_set = data_set(all_val_data)\n",
    "test_set = data_set(all_test_data)\n",
    "  \n",
    "# implementing dataloader on the dataset and printing per batch\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_set, batch_size=1)\n",
    "testloader = DataLoader(test_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d301ed5-5c2d-44ee-846d-44956e1b8607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1247it [02:21,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.41426332621165246\n",
      "Epoch 0: Validation Loss {'Other': 0.02469622237605198, 'Top': 0.5292702094761116, 'VV': 0.9856523657960892, 'VVV': 0.24328871452639003, 'VH': 0.9172820448875427}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1247it [02:26,  8.50it/s]\n",
      "1247it [02:27,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss 0.3865532815074003\n",
      "Epoch 2: Validation Loss {'Other': 0.02343541466743466, 'Top': 0.5270949292035075, 'VV': 0.9846658575702104, 'VVV': 0.2398567288584884, 'VH': 0.8603416681289673}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1247it [02:31,  8.25it/s]\n",
      "1247it [02:25,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss 0.36486936738802705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm   #TODO: Install widgets for notebook version\n",
    "\n",
    "\n",
    "stored_batches = []\n",
    "batch_losses = []\n",
    "epoch_losses = []\n",
    "validation_losses = []\n",
    "if Train:\n",
    "    for epoch in range(num_epochs):\n",
    "        av_epoch_loss = 0\n",
    "        nbatches = 0\n",
    "        model.train()\n",
    "        for idx, (data, weights, samples, sc_weight) in tqdm(enumerate(dataloader, 0)):\n",
    "\n",
    "            data = data.to(device)\n",
    "            #weights = weights.to(device)\n",
    "            sc_weight = sc_weight.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "            out, mu, logVar = model(data)\n",
    "            loss, mse, kld = loss_function(out, data, mu, logVar, variational=model.variational)\n",
    "            #Multiply the loss by the weights\n",
    "            if weight_loss:\n",
    "                loss = torch.dot(added_weight_factor*sc_weight, loss)\n",
    "            else:\n",
    "                loss = torch.sum(loss)\n",
    "\n",
    "            # Backpropagation based on the loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            av_epoch_loss += loss.item()\n",
    "            nbatches += 1\n",
    "            if idx % 5000 == 0:\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "        av_epoch_loss = av_epoch_loss/nbatches\n",
    "        epoch_losses.append(av_epoch_loss)\n",
    "        if epoch % 2 == 0:\n",
    "            print('Epoch {}: Loss {}'.format(epoch, av_epoch_loss))\n",
    "            \n",
    "        #Do a per-sample validation loss\n",
    "        if epoch % val_frequency == 0:\n",
    "            model.eval()\n",
    "            val_losses = {}\n",
    "            val_counts = {}\n",
    "            for idy, (data, weights, samples, sc_weights) in enumerate(valloader, 0):\n",
    "                samples=samples[0]\n",
    "                out, mu, logVar = model(data)\n",
    "                loss, mse, kld = loss_function(out, data, mu, logVar, variational=model.variational)\n",
    "                #Multiply the loss by the weights\n",
    "                if weight_loss:\n",
    "                    loss = torch.dot(added_weight_factor*sc_weights, loss)\n",
    "                else:\n",
    "                    loss = torch.sum(loss)\n",
    "                \n",
    "                running_sum = val_losses.get(reversed_groupings[samples],0) + loss.item()\n",
    "                running_counts = val_losses.get(reversed_groupings[samples],0) + 1\n",
    "                \n",
    "                val_losses[reversed_groupings[samples]] = running_sum\n",
    "                val_counts[reversed_groupings[samples]] = running_counts\n",
    "            #av_val_loss = val_loss/(idy+1)\n",
    "            for key in val_counts.keys():\n",
    "                val_losses[key] = val_losses[key]/val_counts[key]\n",
    "            validation_losses.append(val_losses)\n",
    "            print('Epoch {}: Validation Loss {}'.format(epoch, val_losses))\n",
    "        #Add an early stopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec37e9b-0b5e-41a3-8104-ac911b8be836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "date = dt.datetime.strftime(dt.datetime.now(),\"%H%M-%d-%m-%Y\")\n",
    "output_dir = 'outputs/VAE_'+date\n",
    "if test_dump:\n",
    "    output_dir = 'outputs/test_dump'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "Description = f\"\"\"Training run: {Train}\n",
    "Model variational: {model.variational}\n",
    "Samples used: {chosen_samples}\n",
    "Features used: {training_variables}\n",
    "Number of examples: {len(mc_data)}\n",
    "Weighted loss: {weight_loss}\n",
    "Added weight factor: {added_weight_factor}\n",
    "Learning rate: {learning_rate}\n",
    "Num epochs: {num_epochs}\n",
    "Batch size: {batch_size}\n",
    "Region: {region_file}\n",
    "Number of test samples: {num_test_samples}\n",
    "Validation fraction: {validation_fraction} - {len(all_val_data)} events.\n",
    "Test fraction: {test_fraction} - {len(all_test_data)} events.\n",
    "\n",
    "Model architecture:\n",
    "{model}\n",
    "\n",
    "Optimizer:\n",
    "{optimizer}\n",
    "\n",
    "Additional comments: \n",
    "{additional_comments}\n",
    "\"\"\"\n",
    "with open(os.path.join(output_dir, 'description.txt'),'w') as f:\n",
    "    f.writelines(Description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07331c-5c72-4611-8960-7e7888b1520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model and optimizer\n",
    "if Train:\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, 'model_state_dict.pt'))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750aabb-1a58-4a1d-aea1-e43833228ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64077f6-3201-4682-b5a1-c59c0154fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the scalers used for each column\n",
    "scaler_folder = os.path.join(output_dir, 'scalers')\n",
    "if not os.path.exists(scaler_folder):\n",
    "    os.makedirs(scaler_folder)\n",
    "    \n",
    "import pickle\n",
    "for col, sc in scalers.items():\n",
    "    pickle.dump(sc, open(os.path.join(scaler_folder,col+'_scaler.pkl'),'wb'))\n",
    "    \n",
    "#sc = pickle.load(open('file/path/scaler.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe81506-a5ed-46f1-8f5b-ab28800ad3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if Train:\n",
    "    with torch.no_grad():\n",
    "        plt.scatter([i for i in range(len(batch_losses))], batch_losses)\n",
    "        plt.xlabel('Batch number')\n",
    "        plt.ylabel('Batch Loss')\n",
    "        plt.savefig(os.path.join(output_dir, 'Batch_losses.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667e04d-a29a-42a7-90fa-809028e3155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if Train:\n",
    "    with torch.no_grad():\n",
    "        plt.scatter([i+1 for i in range(len(epoch_losses))], epoch_losses, label='Train')\n",
    "        #plt.scatter([val_frequency*i for i in range(len(validation_losses))], validation_losses, label='Val')\n",
    "        #plot validation per sample\n",
    "        for key in validation_losses[0].keys():\n",
    "            plt.scatter([val_frequency*i+1 for i in range(len(validation_losses))], [v[key] for v in validation_losses], label=key)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Epoch Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, 'Epoch_losses.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9514dbd-a5ac-44fa-87e9-1c5ca5363057",
   "metadata": {},
   "source": [
    "# Set up tracking of training runs\n",
    "\n",
    "## Items to save:\n",
    "\n",
    "Hyperparameters\n",
    "Model architecture \n",
    "Saved model ?\n",
    "Loss plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea53867-5272-48b6-b3a9-d1999581833c",
   "metadata": {},
   "source": [
    "# Start plotting the Anomaly scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf7e0e-e545-4465-b8e3-7770bd70dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Train:\n",
    "    \n",
    "    #Load the model:\n",
    "    model.load_state_dict(torch.load(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183fa45-25b4-4e05-a67c-45cadce8bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# implementing dataloader on the dataset and printing per batch\n",
    "#test_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "# evaluate model:\n",
    "model.eval()\n",
    "\n",
    "test_output = {\n",
    "    'input' : [],\n",
    "    'output' : [],\n",
    "    'mu' : [],\n",
    "    'loss' : [],\n",
    "    'samples' : [],\n",
    "    'groups' : [],\n",
    "    'weights' : [],\n",
    "    'log_losses' : []\n",
    "}\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math \n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (data, weights, sample, sc_weight) in tqdm(enumerate(test_loader, 0)):\n",
    "\n",
    "        # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "        out, mu, logVar = model(data)\n",
    "        loss, mse, kld = loss_function(out, data, mu, logVar, variational=model.variational)\n",
    "        \n",
    "        test_output['input'].append(data)\n",
    "        test_output['output'].append(out)\n",
    "        test_output['mu'].append(mu.tolist())\n",
    "        test_output['loss'].append(loss.item())\n",
    "        test_output['samples'].append(sample[0])\n",
    "        test_output['weights'].append(weights.item())\n",
    "        test_output['log_losses'].append(math.log(loss.item()))\n",
    "        test_output['groups'].append(reversed_groupings[sample[0]])\n",
    "        \n",
    "        if idx > num_test_samples and num_test_samples != -1:\n",
    "            break\n",
    "        \n",
    "test_output['loss'] = np.array(test_output['loss'])\n",
    "test_output['log_losses'] = np.array(test_output['log_losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45885aee-8f03-43d6-b497-d05ecaaba1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/test_mus.npy', np.array(test_output['mu']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89938656-bddd-4a69-8346-e231dce385e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10bf20-89da-4063-a1c6-affb739ffbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(edges, counts, xlabel=None, ylabel=None, title=None, key=None, colour='b', fill=False, save_name=None):\n",
    "    #fig = plt.bar(edges[:-1], counts, width=edges[1]-edges[0], label=key, fill=fill, color=colour, edgecolor=colour)\n",
    "    plt.step(edges[:-1], counts, colour, label=key)\n",
    "    #plt.stairs(counts, edges, color=colour, label=key)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_name is not None:\n",
    "        plt.savefig(save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773cd65-ef8e-4e75-9e42-d2d1c352ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins=50\n",
    "loss_counts, loss_edges = np.histogram(test_output['loss'], bins=num_bins, weights=test_output['weights'])\n",
    "plot_hist(loss_edges, loss_counts, key='All', save_name=os.path.join(output_dir, 'Anomaly_score_hist_All.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276f240-36f0-4d1b-a3b1-9c51cbee78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "num_bins = 50\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(test_output['log_losses'], bins=num_bins, label='All', weights=test_output['weights'],histtype='step')\n",
    "plt.xlabel('Log loss')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'Log_Ascore_All.png'))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4b2ee-4b4d-4349-9ea9-249f485dc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "logloss_bins = np.linspace(-10,10,num=num_bins)\n",
    "\n",
    "test_logloss_counts, test_logloss_edges = np.histogram(test_output['log_losses'], bins=logloss_bins, weights=test_output['weights'])\n",
    "fig = plot_hist(test_logloss_edges, test_logloss_counts, xlabel='Log loss', ylabel='Counts', key='All', save_name=os.path.join(output_dir, 'Log_Ascore_All.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c5bb7-f6d1-4063-8cda-b04c07431dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_hist_stack(edges, counts, xlabel=None, ylabel=None, title=None, labels=None, colours=None, fill=False, save_name=None):\n",
    "\n",
    "    for i, (edge, count) in enumerate(zip(edges,counts)):\n",
    "        #plt.bar(edge[:-1], count, width=edge[1]-edge[0], label=labels[i], fill=fill, edgecolor=colours[i])\n",
    "        plt.step(edge[:-1], count, colours[i], label=labels[i] )\n",
    "        #plt.stairs(count, edge, color=colours[i], label=labels[i])\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_name is not None:\n",
    "        plt.savefig(save_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd25ae4-fe5a-4cb5-811e-aa2db35cf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "edges = []\n",
    "labels = []\n",
    "colour_scheme = {'VV' : 'g', 'VVV' : 'r', 'VH' : 'm' , 'Top': 'b', 'Other' : 'y'}\n",
    "colours = []\n",
    "for group, samples in groupings.items():\n",
    "    index = np.where(np.isin(test_output['samples'],samples))[0]\n",
    "    weights = np.array(test_output['weights'])[index]\n",
    "    loss_counts, loss_edges = np.histogram(test_output['log_losses'][index], bins=logloss_bins, weights=weights)\n",
    "    counts.append(loss_counts)\n",
    "    edges.append(loss_edges)\n",
    "    labels.append(group)\n",
    "    colours.append(colour_scheme[group])\n",
    "    \n",
    "fig = plot_hist_stack(edges, counts, xlabel='Log loss', ylabel='Counts', labels=labels, colours=colours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e2e7d-2303-4182-9a85-5be76104e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "fig = plt.figure()\n",
    "for group, samples in groupings.items():\n",
    "    bar_heights = 0\n",
    "    index = np.where(np.isin(test_output['samples'],samples))\n",
    "    weights = np.array(test_output['weights'])[index]\n",
    "    plt.hist(np.array(test_output['log_losses'])[index], bins=logloss_bins, label=group, weights=weights,histtype='step')\n",
    "    \n",
    "plt.xlabel('Log loss')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, 'Anomaly_score_hist_bySample.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4911786-92d5-49a4-bd66-66ae28f2f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for group, samples in groupings.items():\n",
    "    bar_heights = 0\n",
    "    index = np.where(np.isin(test_output['samples'],samples))\n",
    "    weights = np.array(test_output['weights'])[index] / np.sum(np.array(test_output['weights'])[index])\n",
    "    plt.hist(np.array(test_output['log_losses'])[index], bins=logloss_bins, label=group, weights=weights,histtype='step')\n",
    "    \n",
    "plt.xlabel('Log loss')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'Anomaly_score_hist_bySampleNormalised.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344410d4-ab00-4e10-9926-8ffc7f9f6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, samples in groupings.items():\n",
    "    fig = plt.figure()\n",
    "    bar_heights = 0\n",
    "    index = np.where(np.isin(test_output['samples'],samples))\n",
    "    weights = np.array(test_output['weights'])[index]\n",
    "    plt.hist(np.array(test_output['log_losses'])[index], bins=logloss_bins, label=group, weights=weights,histtype='step')\n",
    "    plt.xlabel('Log loss')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, f\"Anomaly_score_hist_Sample{group}.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda47c70-6998-496b-bef7-5443f60aff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_dir, 'test_mus.npy'), np.array(test_output['mu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512e725-9dd7-49cd-a2ee-c88c170ada41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "def plot_umap(fit_data, targets=None,labs=None, neighbours=15, rand_state=None, title=None, colour=\"gist_rainbow\",\n",
    "              save_name=None):\n",
    "    \n",
    "    fit = umap.UMAP(random_state=rand_state, n_neighbors=neighbours)\n",
    "    u = fit.fit_transform(fit_data)\n",
    "    fig,ax = plt.subplots(figsize=(10,8))\n",
    "    cmap = plt.get_cmap(colour,10) #put ,10) for discrete blocks  #colour = viridis\n",
    "    if type(targets) != type(None):\n",
    "        if type(targets[0]) == str:\n",
    "            cax = ax.scatter(u[:,0], u[:,1], s=5, c=targets, label=targets)#, cmap=cmap)\n",
    "            #fig.colorbar(cax)\n",
    "        else:\n",
    "            cax = ax.scatter(u[:,0], u[:,1], s=5, c=targets, cmap=cmap, \n",
    "                     vmin=targets.min(), vmax=targets.max())\n",
    "            fig.colorbar(cax)#, extend='min')\n",
    "    else:\n",
    "        cax = ax.scatter(u[:,0], u[:,1], s=5)\n",
    "    plt.legend(handles=cax.legend_elements()[0])\n",
    "    plt.title(title)\n",
    "    if save_name != None:\n",
    "        print(\"Saving...\")\n",
    "        plt.savefig(fname=f\"{save_name}\", quality=100)\n",
    "    plt.show()\n",
    "    \n",
    "plot_umap(mus.reshape(5002,-1), neighbours=5, targets=gs, labs=labs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292b0f4-2123-4137-b730-64477db76e51",
   "metadata": {},
   "source": [
    "# Signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbad84-998e-46a8-9dc2-144315913767",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_signals:\n",
    "    \n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    signal_data = pd.read_feather(os.path.join(all_samples_path, signal_feather))\n",
    "    \n",
    "    \n",
    "    #Calculate weights\n",
    "    total_lum = 138965.16\n",
    "    signal_data.loc[signal_data['RunYear'].isin([2015,2016]), 'lumi_scale'] = 36207.66*(1/total_lum)\n",
    "    signal_data.loc[signal_data['RunYear'].isin([2017]), 'lumi_scale'] = 44307.4*(1/total_lum)\n",
    "    signal_data.loc[signal_data['RunYear'].isin([2018]), 'lumi_scale'] = 58450.1*(1/total_lum)\n",
    "\n",
    "    signal_data['weight'] = signal_data['lumi_scale']*signal_data['custTrigSF_TightElMediumMuID_FCLooseIso_DLT']*signal_data['weight_pileup']*signal_data['jvtSF_customOR']*signal_data['bTagSF_weight_DL1r_77']*signal_data['weight_mc']*signal_data['xs']*signal_data['lep_SF_CombinedLoose_0']*signal_data['lep_SF_CombinedLoose_1']*signal_data['lep_SF_CombinedLoose_2']*signal_data['lep_SF_CombinedLoose_3']/signal_data['totalEventsWeighted']\n",
    "\n",
    "    #Select columns\n",
    "    signal_data = signal_data[training_variables+['VLL_type', 'VLL_decaytype']]\n",
    "    signal_sample_types = list(signal_data['sample'].unique())\n",
    "    \n",
    "    if get_bestZ:\n",
    "        signal_data = get_Zll_pairing(signal_data, drop_Mlls=True)\n",
    "    \n",
    "    #separation_samples = ['Esinglet300', 'Mdoublet700']\n",
    "    #signal_data = signal_data.loc[signal_data['sample'].isin(separation_samples)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7459d-203b-427e-8bbb-b6daa311a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ae35f-f4b4-433e-bd9d-f62fa0f077fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_signals:\n",
    "    #Scale values by the same way \n",
    "    for col in signal_data.columns:\n",
    "\n",
    "        \n",
    "        if col == 'weight':\n",
    "            #Scale the weights to be centered on 1\n",
    "            signal_data.loc[:,'scaled_weight'] = signal_data.loc[:,col]/signal_data[col].sum()\n",
    "            continue\n",
    "\n",
    "        if col in ['sample','VLL_type', 'VLL_decaytype']:\n",
    "            continue\n",
    "        \n",
    "        sc = scalers[col]\n",
    "        signal_data.loc[:, col] = sc.transform(np.array(signal_data[col]).reshape(len(signal_data[col]),1))\n",
    "        print(col, signal_data[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d79ac0-f0c2-4e10-8fd7-ea7e4d8f0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_signals:\n",
    "    # implementing dataloader on the dataset and printing per batch\n",
    "    sig_dataset = data_set(signal_data)\n",
    "    signal_loader = DataLoader(sig_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # evaluate model:\n",
    "    model.eval()\n",
    "\n",
    "    sig_output = {\n",
    "        'input' : [],\n",
    "        'output' : [],\n",
    "        'mu' : [],\n",
    "        'loss' : [],\n",
    "        'samples' : [],\n",
    "        'groups' : [],\n",
    "        'weights' : [],\n",
    "        'log_losses' : [],\n",
    "        'vll_type' : [],\n",
    "        'vll_decaytype' : []\n",
    "    }\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    import math \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, weights, sample, sc_weights) in tqdm(enumerate(signal_loader, 0)):\n",
    "\n",
    "            types = data[:,-2:]\n",
    "            sig_output['vll_type'].append(types[0,0].item())\n",
    "            sig_output['vll_decaytype'].append(types[0,1].item())\n",
    "            data = data[:,:-2]\n",
    "            # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "            out, mu, logVar = model(data)\n",
    "            loss, mse, kld = loss_function(out, data, mu, logVar, variational=model.variational)\n",
    "\n",
    "            sig_output['input'].append(data)\n",
    "            sig_output['output'].append(out)\n",
    "            sig_output['mu'].append(mu.tolist())\n",
    "            sig_output['loss'].append(loss.item())\n",
    "            sig_output['samples'].append(sample[0])\n",
    "            sig_output['weights'].append(weights.item())\n",
    "            sig_output['log_losses'].append(math.log(loss.item()))\n",
    "            #sig_output['groups'].append(reversed_groupings[sample[0]])\n",
    "\n",
    "            if idx > num_test_samples and num_test_samples != -1:\n",
    "                break\n",
    "                \n",
    "    sig_output['loss'] = np.array(sig_output['loss'])\n",
    "    sig_output['log_losses'] = np.array(sig_output['log_losses'])\n",
    "    np.save(os.path.join(output_dir, 'sig_mus.npy'), np.array(sig_output['mu']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0a680-afca-456a-b31a-11bccb79aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_signals:\n",
    "    plt.hist(sig_output['log_losses'], bins=logloss_bins, label='All', weights=sig_output['weights'],histtype='step')\n",
    "    plt.xlabel('Log loss')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'Log_Ascore_SignalsAll.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723d5da-ebc9-4df0-9148-eb7bfff5fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "fig = plt.figure()\n",
    "\n",
    "for group, samples in groupings.items():\n",
    "    bar_heights = 0\n",
    "    index = np.where(np.isin(test_output['samples'],samples))\n",
    "    weights = np.array(test_output['weights'])[index] / np.sum(np.array(test_output['weights'])[index])\n",
    "    plt.hist(np.array(test_output['log_losses'])[index], bins=logloss_bins, label=group, weights=weights,histtype='step')\n",
    "    \n",
    "if evaluate_signals:\n",
    "    weights2 = np.array(sig_output['weights']) / np.sum(np.array(sig_output['weights']))\n",
    "    plt.hist(sig_output['log_losses'], bins=logloss_bins, label='VLLs', weights=weights2,histtype='step')\n",
    "    \n",
    "plt.xlabel('Log loss')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'Anomaly_score_hist_bySampleNormalised_VLL.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9f01e-c615-4403-8be8-30d12caa17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "plt.hist(test_output['log_losses'], bins=logloss_bins, label='Bkg', weights=test_output['weights']/np.sum(test_output['weights']),histtype='step')\n",
    "if evaluate_signals:\n",
    "    plt.hist(sig_output['log_losses'], bins=logloss_bins, label='Sig', weights=sig_output['weights']/np.sum(sig_output['weights']),histtype='step')\n",
    "plt.xlabel('Log loss')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'Log_Ascore_BkgvsSig_50bins.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe52ecc-e88f-470b-8903-c69220031702",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_signals:\n",
    "    num_bins = 50\n",
    "    for sample in signal_sample_types:\n",
    "        index = np.where(np.isin(sig_output['samples'],[sample]))\n",
    "        weights = np.array(sig_output['weights'])[index] / np.sum(np.array(sig_output['weights'])[index])\n",
    "        plt.hist(np.array(sig_output['log_losses'])[index], bins=logloss_bins, label=sample, weights=weights,histtype='step')\n",
    "    \n",
    "    weights2 = np.array(test_output['weights']) / np.sum(np.array(test_output['weights']))\n",
    "    plt.hist(test_output['log_losses'], bins=logloss_bins, label='Bkg', weights=weights2,histtype='step')\n",
    "    plt.xlabel('Log loss')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'SignalSample_breakdown.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688fc54-f03b-461f-b849-448bbef45758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a low mass and a high mass separation\n",
    "\n",
    "def get_chi2distance(x,y):\n",
    "    \n",
    "    ch2 = np.nan_to_num(((x-y)**2)/(x+y), copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "    ch2 = 0.5 * np.sum(ch2)\n",
    "    return ch2\n",
    "\n",
    "\n",
    "separation_samples = ['Esinglet300', 'Mdoublet700']\n",
    "\n",
    "if evaluate_signals:\n",
    "    \n",
    "    histos = []\n",
    "    edges = []\n",
    "    test_logloss_counts, test_logloss_edges = np.histogram(test_output['log_losses'], bins=logloss_bins, weights=test_output['weights']/np.sum(test_output['weights']))\n",
    "    \n",
    "    for sample in separation_samples:\n",
    "        index = np.where(np.isin(sig_output['samples'],[sample]))\n",
    "        weights = np.array(sig_output['weights'])[index] /  np.sum(np.array(sig_output['weights'])[index])\n",
    "        loss_counts, loss_edges = np.histogram(sig_output['log_losses'][index], bins=logloss_bins, weights=weights)\n",
    "        histos.append(loss_counts)\n",
    "        edges.append(loss_edges)\n",
    "\n",
    "    \n",
    "    chi2_out = []\n",
    "    for i, sample in enumerate(separation_samples):\n",
    "        chi2 = get_chi2distance(test_logloss_counts, histos[i])\n",
    "        out_str = f\"Histogram: {sample}, chi2 distance from background: {chi2}\"\n",
    "        print(out_str)\n",
    "        chi2_out.append(out_str + '\\n')\n",
    "    histos.append(test_logloss_counts)\n",
    "    edges.append(test_logloss_edges)\n",
    "    with open(os.path.join(output_dir, 'Chi2_Distances.txt'),'w') as f:\n",
    "        f.writelines(chi2_out)\n",
    "        \n",
    "    savename = os.path.join(output_dir, 'Separation_Hist.png')\n",
    "    fig = plot_hist_stack(edges, histos, xlabel='Log loss', colours=['r','g','b'], ylabel='Counts', labels=['E(300)','M(700)','Bkg'], save_name=savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6aad7e-8011-454c-bf75-9f1b167865b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot by VLL_type\n",
    "if evaluate_signals:\n",
    "    \n",
    "    histos = []\n",
    "    edges = []\n",
    "    types = []\n",
    "    \n",
    "    #ONLY PLOT E(300)\n",
    "        \n",
    "    for vll_type in set(sig_output['vll_type']):\n",
    "        index1 = np.where(np.isin(sig_output['samples'],[separation_samples[0]]))\n",
    "        index2 = np.where(np.isin(np.array(sig_output['vll_type'])[index1],[vll_type]))\n",
    "        sum_W = np.sum(np.array(sig_output['weights'])[index1][index2])\n",
    "        if sum_W == 0:\n",
    "            weights=np.zeros(shape=np.array(sig_output['weights'])[index1][index2].shape)\n",
    "        else:\n",
    "            weights = np.array(sig_output['weights'])[index1][index2] /  sum_W\n",
    "        loss_counts, loss_edges = np.histogram(sig_output['log_losses'][index1][index2], bins=logloss_bins, weights=weights)\n",
    "        histos.append(loss_counts)\n",
    "        edges.append(loss_edges)\n",
    "        types.append(vll_type)\n",
    "        \n",
    "    histos.append(test_logloss_counts)\n",
    "    edges.append(test_logloss_edges)\n",
    "    \n",
    "    savename = os.path.join(output_dir, 'VLL_type_breakdown.png')\n",
    "    fig = plot_hist_stack(edges, histos, xlabel='Log loss', colours=['r','g','y','b'], ylabel='Counts', labels=['LL','NL','NN','Bkg'], save_name=savename)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac862c-6fba-4367-b48f-b9b012b96bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot by VLL_decaytype\n",
    "if evaluate_signals:\n",
    "    \n",
    "    histos = []\n",
    "    edges = []\n",
    "    types = []\n",
    "        \n",
    "    for vll_decaytype in set(sig_output['vll_decaytype']):\n",
    "        \n",
    "        index1 = np.where(np.isin(sig_output['samples'],[separation_samples[0]]))\n",
    "        index2 = np.where(np.isin(np.array(sig_output['vll_decaytype'])[index1],[vll_decaytype]))\n",
    "        weights = np.array(sig_output['weights'])[index1][index2] /  np.sum(np.array(sig_output['weights'])[index1][index2])\n",
    "        loss_counts, loss_edges = np.histogram(sig_output['log_losses'][index1][index2], bins=logloss_bins, weights=weights)\n",
    "        histos.append(loss_counts)\n",
    "        edges.append(loss_edges)\n",
    "        types.append(vll_decaytype)\n",
    "        \n",
    "    histos.append(test_logloss_counts)\n",
    "    edges.append(test_logloss_edges)\n",
    "    \n",
    "    savename = os.path.join(output_dir, 'VLL_decaytype.png')\n",
    "    fig = plot_hist_stack(edges, histos, xlabel='Log loss', colours=['r','g','y','c','m','k','b'], ylabel='Counts', labels=['ZZ','HH','WW','HZ','WZ','WH','Bkg'], save_name=savename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a709697-d5cd-4720-8be4-f5e6533ce09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c515f1-6c36-441a-ad36-c54a74c85426",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = [m[0][0] for m in test_output['mu']]\n",
    "mu_1 = [m[0][1] for m in test_output['mu']]\n",
    "sig_mu0 = [m[0][0] for m in sig_output['mu']]\n",
    "sig_mu1 = [m[0][1] for m in sig_output['mu']]\n",
    "\n",
    "plt.scatter(sig_mu0, sig_mu1, label='Sig')\n",
    "plt.scatter(mu_0, mu_1, label='Bkg')\n",
    "\n",
    "plt.xlabel('z1')\n",
    "plt.ylabel('z2')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'LatentScatter_SigvsBkg.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de329203-5331-4df5-a0c0-712ae7d2ae48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac46f6-1014-4f24-83ec-25f3c03f4b59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_kernel",
   "language": "python",
   "name": "ml_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
